---
title: Foundational RAG Datasets
description: Core benchmarks that established the foundations of RAG evaluation - Natural Questions, MS MARCO, HotpotQA, and multi-hop reasoning datasets
tags: [evaluation, datasets, natural-questions, ms-marco, hotpotqa]
lastUpdated: 2025-08-24
---

<svelte:head>
  <title>Foundational RAG Datasets â€¢ Surreal Agent Memory</title>
  <meta name="description" content="Core benchmarks that established the foundations of RAG evaluation - Natural Questions, MS MARCO, HotpotQA, and multi-hop reasoning datasets" />
</svelte:head>

# ðŸ“š Foundational RAG Datasets

<p class="text-gray-600 leading-relaxed">The evolution of Retrieval-Augmented Generation has been built upon a series of foundational datasets, each addressing specific challenges in natural language processing and information retrieval.</p>

## Open-Domain Question Answering

### Natural Questions (NQ)

<div class="bg-blue-50 border border-blue-200 rounded-xl p-5 mb-6">
  <div class="flex items-start gap-3">
    <div class="bg-blue-100 rounded-lg p-2">
      <svg class="w-5 h-5 text-blue-600" fill="currentColor" viewBox="0 0 20 20">
        <path d="M9 12l2 2 4-4m6 2a9 9 0 11-18 0 9 9 0 0118 0z"/>
      </svg>
    </div>
    <div>
      <div class="font-semibold text-blue-900 mb-2">Real-World Query Authenticity</div>
      <p class="text-sm text-blue-800">Natural Questions consists of real, anonymized user queries issued to Google search engine, providing genuine representation of human information-seeking behavior.</p>
    </div>
  </div>
</div>

**Key Features:**
- **Source**: Real Google search queries paired with Wikipedia answers
- **Scale**: 300,000+ training examples with single annotations
- **Structure**: Both "long answers" (HTML bounding boxes) and "short answers" (specific entities)
- **Validation**: Development and test sets with five-way annotations for robust evaluation

**Answer Structure:**
- **Long Answer**: Smallest HTML bounding box (paragraph, list, table row) containing all necessary information
- **Short Answer**: One or more specific entities that directly answer the question
- **Annotation Quality**: Human annotators ensure high-quality ground truth

<div class="bg-white border border-gray-200 rounded-lg p-4 my-4">
  <div class="text-sm font-medium text-gray-700 mb-2">ðŸ“Š Primary Metrics</div>
  <div class="grid grid-cols-2 gap-4 text-sm">
    <div>
      <span class="font-medium">F1 Score:</span> Harmonic mean of precision and recall
    </div>
    <div>
      <span class="font-medium">Exact Match:</span> Binary correctness assessment
    </div>
  </div>
</div>

### MS MARCO

<div class="bg-green-50 border border-green-200 rounded-xl p-5 mb-6">
  <div class="flex items-start gap-3">
    <div class="bg-green-100 rounded-lg p-2">
      <svg class="w-5 h-5 text-green-600" fill="currentColor" viewBox="0 0 20 20">
        <path d="M3 4a1 1 0 011-1h12a1 1 0 011 1v2a1 1 0 01-1 1H4a1 1 0 01-1-1V4zM3 10a1 1 0 011-1h6a1 1 0 011 1v6a1 1 0 01-1 1H4a1 1 0 01-1-1v-6zM14 9a1 1 0 00-1 1v6a1 1 0 001 1h2a1 1 0 001-1v-6a1 1 0 00-1-1h-2z"/>
      </svg>
    </div>
    <div>
      <div class="font-semibold text-green-900 mb-2">Large-Scale Information Retrieval</div>
      <p class="text-sm text-green-800">Microsoft MAchine Reading COmprehension focuses on scaling information retrieval tasks with millions of documents and passages from real Bing queries.</p>
    </div>
  </div>
</div>

**Key Features:**
- **Source**: Real Bing user queries with web document corpus
- **Scale**: Millions of documents and passages
- **Focus**: Passage and document ranking evaluation
- **Purpose**: Independent assessment of retrieval component

**Specialized Tasks:**
- **Passage Ranking**: Standard benchmark for cross-encoder training
- **Document Ranking**: Large-scale document collection handling
- **Retrieval Optimization**: Efficient search system development

<div class="bg-white border border-gray-200 rounded-lg p-4 my-4">
  <div class="text-sm font-medium text-gray-700 mb-2">ðŸ“Š Primary Metrics</div>
  <div class="grid grid-cols-2 gap-4 text-sm">
    <div>
      <span class="font-medium">NDCG@10:</span> Normalized Discounted Cumulative Gain
    </div>
    <div>
      <span class="font-medium">MRR@10:</span> Mean Reciprocal Rank
    </div>
  </div>
</div>

## Multi-Hop Reasoning Datasets

### HotpotQA

<div class="bg-purple-50 border border-purple-200 rounded-xl p-5 mb-6">
  <div class="flex items-start gap-3">
    <div class="bg-purple-100 rounded-lg p-2">
      <svg class="w-5 h-5 text-purple-600" fill="currentColor" viewBox="0 0 20 20">
        <path d="M13.586 3.586a2 2 0 112.828 2.828l-.793.793-2.828-2.828.793-.793zM11.379 5.793L3 14.172V17h2.828l8.38-8.379-2.83-2.828z"/>
      </svg>
    </div>
    <div>
      <div class="font-semibold text-purple-900 mb-2">Multi-Document Reasoning</div>
      <p class="text-sm text-purple-800">Designed to challenge models' ability to find and synthesize information from multiple supporting documents to form complete answers.</p>
    </div>
  </div>
</div>

**Unique Innovation: Supporting Facts**
- **Explicit Supervision**: Specific sentences within source documents that justify the answer
- **Explainability**: Enables evaluation of reasoning path, not just final answer
- **Quality Assurance**: Ensures models follow correct logical progression

**Evaluation Settings:**
1. **Distractor Setting**: Choose from limited set of 10 paragraphs
2. **Fullwiki Setting**: Open-domain retrieval from entire Wikipedia corpus

<div class="bg-white border border-gray-200 rounded-lg p-4 my-4">
  <div class="text-sm font-medium text-gray-700 mb-2">ðŸ“Š Primary Metrics</div>
  <div class="grid grid-cols-3 gap-4 text-sm">
    <div>
      <span class="font-medium">F1 Score:</span> Answer accuracy
    </div>
    <div>
      <span class="font-medium">Exact Match:</span> Binary correctness
    </div>
    <div>
      <span class="font-medium">Supporting Facts:</span> Reasoning path validation
    </div>
  </div>
</div>

### 2WikiMultiHopQA

<div class="bg-orange-50 border border-orange-200 rounded-xl p-5 mb-6">
  <div class="flex items-start gap-3">
    <div class="bg-orange-100 rounded-lg p-2">
      <svg class="w-5 h-5 text-orange-600" fill="currentColor" viewBox="0 0 20 20">
        <path d="M9 12l2 2 4-4m6 2a9 9 0 11-18 0 9 9 0 0118 0z"/>
      </svg>
    </div>
    <div>
      <div class="font-semibold text-orange-900 mb-2">Guaranteed Multi-Hop Reasoning</div>
      <p class="text-sm text-orange-800">Addresses limitations in previous datasets by guaranteeing each question genuinely requires multi-hop steps to be answered.</p>
    </div>
  </div>
</div>

**Key Improvements:**
- **Complete Explanation**: Full reasoning process documentation
- **Genuine Multi-Hop**: Questions cannot be answered without true multi-hop reasoning
- **Template-Based Design**: Logical rules ensure question complexity
- **Structured Data Integration**: Leverages Wikidata for natural question generation

**Technical Innovation:**
- **Pipeline Design**: Careful template and rule construction
- **Quality Control**: Eliminates questions answerable through single-hop reasoning
- **Data Sources**: Combines structured (Wikidata) and unstructured (Wikipedia) information

## Dataset Comparison Matrix

<div class="overflow-x-auto my-8">
  <table class="min-w-full bg-white border border-gray-200 rounded-lg">
    <thead class="bg-gray-50">
      <tr>
        <th class="px-4 py-3 text-left text-xs font-medium text-gray-500 uppercase">Dataset</th>
        <th class="px-4 py-3 text-left text-xs font-medium text-gray-500 uppercase">Primary Purpose</th>
        <th class="px-4 py-3 text-left text-xs font-medium text-gray-500 uppercase">Source Domain</th>
        <th class="px-4 py-3 text-left text-xs font-medium text-gray-500 uppercase">Key Features</th>
        <th class="px-4 py-3 text-left text-xs font-medium text-gray-500 uppercase">Primary Metrics</th>
      </tr>
    </thead>
    <tbody class="divide-y divide-gray-200">
      <tr>
        <td class="px-4 py-3 text-sm font-medium text-gray-900">Natural Questions</td>
        <td class="px-4 py-3 text-sm text-gray-600">Open-domain QA</td>
        <td class="px-4 py-3 text-sm text-gray-600">Google Search, Wikipedia</td>
        <td class="px-4 py-3 text-sm text-gray-600">Real user questions; long/short answers</td>
        <td class="px-4 py-3 text-sm text-gray-600">F1, Exact Match</td>
      </tr>
      <tr>
        <td class="px-4 py-3 text-sm font-medium text-gray-900">MS MARCO</td>
        <td class="px-4 py-3 text-sm text-gray-600">Information Retrieval</td>
        <td class="px-4 py-3 text-sm text-gray-600">Bing Search, Web Docs</td>
        <td class="px-4 py-3 text-sm text-gray-600">Large scale (~8.8M passages)</td>
        <td class="px-4 py-3 text-sm text-gray-600">MRR@10, NDCG@10</td>
      </tr>
      <tr>
        <td class="px-4 py-3 text-sm font-medium text-gray-900">HotpotQA</td>
        <td class="px-4 py-3 text-sm text-gray-600">Multi-hop Reasoning</td>
        <td class="px-4 py-3 text-sm text-gray-600">Wikipedia</td>
        <td class="px-4 py-3 text-sm text-gray-600">Supporting facts for explainability</td>
        <td class="px-4 py-3 text-sm text-gray-600">F1, EM, Supporting Facts</td>
      </tr>
      <tr>
        <td class="px-4 py-3 text-sm font-medium text-gray-900">2WikiMultiHopQA</td>
        <td class="px-4 py-3 text-sm text-gray-600">Guaranteed Multi-hop</td>
        <td class="px-4 py-3 text-sm text-gray-600">Wikipedia, Wikidata</td>
        <td class="px-4 py-3 text-sm text-gray-600">Template-based, complete explanation</td>
        <td class="px-4 py-3 text-sm text-gray-600">F1, Exact Match</td>
      </tr>
    </tbody>
  </table>
</div>

## Evolution of Truth Definition

The progression of these datasets illustrates a fundamental evolution in defining "correctness":

<div class="space-y-4 my-6">
  <div class="border-l-4 border-blue-500 pl-4">
    <div class="font-medium text-gray-900">Phase 1: Binary Ground Truth</div>
    <p class="text-sm text-gray-600 mt-1">Natural Questions defined truth as single, human-annotated answers - straightforward but limited.</p>
  </div>

  <div class="border-l-4 border-green-500 pl-4">
    <div class="font-medium text-gray-900">Phase 2: Justified Reasoning</div>
    <p class="text-sm text-gray-600 mt-1">HotpotQA required models to cite supporting facts, linking answer quality to retrieval quality.</p>
  </div>

  <div class="border-l-4 border-purple-500 pl-4">
    <div class="font-medium text-gray-900">Phase 3: Diagnostic Assessment</div>
    <p class="text-sm text-gray-600 mt-1">Modern benchmarks provide multi-faceted evaluation with actionable feedback for specific failure modes.</p>
  </div>
</div>

<div class="bg-gray-50 border border-gray-200 rounded-xl p-4 mt-8">
  <div class="text-sm font-medium text-gray-700 mb-2">ðŸ”— Related Resources</div>
  <div class="grid md:grid-cols-2 gap-4 text-sm">
    <a href="/docs/research/evaluation-datasets/modern-benchmarks" class="text-blue-600 hover:underline">â†’ Modern Benchmarks & Specialized Datasets</a>
    <a href="/docs/research/evaluation-datasets/metrics" class="text-blue-600 hover:underline">â†’ Evaluation Metrics & Methodology</a>
  </div>
</div>
